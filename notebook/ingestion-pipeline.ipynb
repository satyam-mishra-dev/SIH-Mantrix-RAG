{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d96bea1",
   "metadata": {},
   "source": [
    "# RAG Model Data Ingestion & Cleaning Pipeline\n",
    "\n",
    "This notebook processes and cleans data from multiple sources for the RAG model:\n",
    "- **dataset.csv**: Career outcome data (59 student profiles)\n",
    "- **holland-codes.txt**: Holland's RIASEC career theory documentation\n",
    "- **lifespan.txt**: Super's Life-Span, Life-Space theory documentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "208d3e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "# !pip install langchain langchain-community langchain-core pandas numpy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import CSVLoader, TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b62a473d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents loaded: 59\n",
      "First document preview: Age: 23\n",
      "Education_Level: Bachelor's\n",
      "Degree_Field: Computer Science\n",
      "GPA: 3.8\n",
      "Technical_Skills: Python;Java;SQL;AWS\n",
      "Soft_Skills: Communication;Teamwork;Problem-Solving\n",
      "Personality_Traits: Analytical\n",
      "Lea...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##CSV FILE LOADING\n",
    "\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "loader = CSVLoader(\"../data/dataset.csv\")\n",
    "csv_documents = loader.load()\n",
    "\n",
    "print(f\"Number of documents loaded: {len(csv_documents)}\")\n",
    "print(f\"First document preview: {csv_documents[0].page_content[:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce56e533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents loaded: 1\n",
      "First document preview: Life-Span, Life-Space Theory\n",
      "The Life-Span, Life-Space Theory is a prominent model in vocational psychology developed by American psychologist Donald Super. It emphasizes that career development is a ...\n",
      "Number of documents loaded: 1\n",
      "First document preview: The Holland Codes or the Holland Occupational Themes (RIASEC[1]) are a taxonomy of interests[2] based on a theory of careers and vocational choice that was initially developed by American psychologist...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"../data/lifespan.txt\")\n",
    "lifespan_documents = loader.load()\n",
    "\n",
    "print(f\"Number of documents loaded: {len(lifespan_documents)}\")\n",
    "print(f\"First document preview: {lifespan_documents[0].page_content[:200]}...\")\n",
    "\n",
    "loader = TextLoader(\"../data/holland-codes.txt\")\n",
    "holland_documents = loader.load()\n",
    "\n",
    "print(f\"Number of documents loaded: {len(holland_documents)}\")\n",
    "print(f\"First document preview: {holland_documents[0].page_content[:200]}...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e9aa3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning functions\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and normalize text content\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return str(text)\n",
    "    \n",
    "    # Remove extra whitespace and normalize\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove special characters but keep basic punctuation\n",
    "    text = re.sub(r'[^\\w\\s.,!?;:()\\-]', '', text)\n",
    "    # Remove multiple periods, commas, etc.\n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    text = re.sub(r',{2,}', ',', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def clean_csv_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean CSV data for better processing\"\"\"\n",
    "    # Create a copy to avoid modifying original\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Clean text columns\n",
    "    text_columns = ['Technical_Skills', 'Soft_Skills', 'Personality_Traits', \n",
    "                   'Past_Projects', 'Extracurriculars', 'Career_Outcome']\n",
    "    \n",
    "    for col in text_columns:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].apply(clean_text)\n",
    "    \n",
    "    # Clean semicolon-separated lists\n",
    "    list_columns = ['Technical_Skills', 'Soft_Skills']\n",
    "    for col in list_columns:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].apply(\n",
    "                lambda x: '; '.join([skill.strip() for skill in str(x).split(';') if skill.strip()])\n",
    "            )\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def process_text_file(file_path: str) -> str:\n",
    "    \"\"\"Process and clean text files\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Clean the content\n",
    "    content = clean_text(content)\n",
    "    \n",
    "    # Split into sections for better chunking\n",
    "    sections = re.split(r'\\n\\n+', content)\n",
    "    cleaned_sections = [section.strip() for section in sections if section.strip()]\n",
    "    \n",
    "    return '\\n\\n'.join(cleaned_sections)\n",
    "\n",
    "print(\"Data cleaning functions defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7a1d949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROCESSING CSV DATA ===\n",
      "Original CSV shape: (59, 13)\n",
      "Columns: ['Age', 'Education_Level', 'Degree_Field', 'GPA', 'Technical_Skills', 'Soft_Skills', 'Personality_Traits', 'Learning_Style', 'Work_Style_Preference', 'Past_Projects', 'Internships_Completed', 'Extracurriculars', 'Career_Outcome']\n",
      "Cleaned CSV shape: (59, 13)\n",
      "\n",
      "Sample of cleaned data:\n",
      "   Age             Degree_Field                         Technical_Skills  \\\n",
      "0   23         Computer Science                   Python; Java; SQL; AWS   \n",
      "1   25             Data Science  Python; R; Machine Learning; TensorFlow   \n",
      "2   22  Business Administration                      Excel; SQL; Tableau   \n",
      "\n",
      "                           Career_Outcome  \n",
      "0                Data Scientist at Google  \n",
      "1  Machine Learning Engineer at Microsoft  \n",
      "2             Marketing Analyst at Amazon  \n",
      "\n",
      "Missing values per column:\n",
      "Age                      0\n",
      "Education_Level          0\n",
      "Degree_Field             0\n",
      "GPA                      0\n",
      "Technical_Skills         0\n",
      "Soft_Skills              0\n",
      "Personality_Traits       0\n",
      "Learning_Style           0\n",
      "Work_Style_Preference    0\n",
      "Past_Projects            0\n",
      "Internships_Completed    0\n",
      "Extracurriculars         0\n",
      "Career_Outcome           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load and clean CSV data\n",
    "print(\"=== PROCESSING CSV DATA ===\")\n",
    "df = pd.read_csv(\"../data/dataset.csv\")\n",
    "print(f\"Original CSV shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# Clean the data\n",
    "df_clean = clean_csv_data(df)\n",
    "print(f\"Cleaned CSV shape: {df_clean.shape}\")\n",
    "\n",
    "# Display sample of cleaned data\n",
    "print(\"\\nSample of cleaned data:\")\n",
    "print(df_clean[['Age', 'Degree_Field', 'Technical_Skills', 'Career_Outcome']].head(3))\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values per column:\")\n",
    "print(df_clean.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0eb03d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string expression part cannot include a backslash (4213118502.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"Holland Codes - Number of sections: {len(holland_content.split('\\\\n\\\\n'))}\")\u001b[0m\n\u001b[0m                                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string expression part cannot include a backslash\n"
     ]
    }
   ],
   "source": [
    "# Process text files\n",
    "print(\"\\n=== PROCESSING TEXT FILES ===\")\n",
    "\n",
    "# Process Holland Codes\n",
    "holland_content = process_text_file(\"../data/holland-codes.txt\")\n",
    "print(f\"Holland Codes - Original length: {len(open('../data/holland-codes.txt').read())}\")\n",
    "print(f\"Holland Codes - Cleaned length: {len(holland_content)}\")\n",
    "# Fix the f-string syntax error by using a variable for the split pattern\n",
    "split_pattern = '\\n\\n'\n",
    "print(f\"Holland Codes - Number of sections: {len(holland_content.split(split_pattern))}\")\n",
    "\n",
    "# Process Lifespan theory\n",
    "lifespan_content = process_text_file(\"../data/lifespan.txt\")\n",
    "print(f\"Lifespan Theory - Original length: {len(open('../data/lifespan.txt').read())}\")\n",
    "print(f\"Lifespan Theory - Cleaned length: {len(lifespan_content)}\")\n",
    "print(f\"Lifespan Theory - Number of sections: {len(lifespan_content.split(split_pattern))}\")\n",
    "\n",
    "print(\"\\nSample of cleaned Holland Codes content:\")\n",
    "print(holland_content[:300] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d8f7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LangChain documents for RAG\n",
    "print(\"\\n=== CREATING LANGCHAIN DOCUMENTS ===\")\n",
    "\n",
    "# Convert CSV data to documents\n",
    "csv_documents = []\n",
    "for idx, row in df_clean.iterrows():\n",
    "    # Create a comprehensive document for each student profile\n",
    "    content = f\"\"\"\n",
    "    Student Profile {idx + 1}:\n",
    "    Age: {row['Age']}\n",
    "    Education Level: {row['Education_Level']}\n",
    "    Degree Field: {row['Degree_Field']}\n",
    "    GPA: {row['GPA']}\n",
    "    Technical Skills: {row['Technical_Skills']}\n",
    "    Soft Skills: {row['Soft_Skills']}\n",
    "    Personality Traits: {row['Personality_Traits']}\n",
    "    Learning Style: {row['Learning_Style']}\n",
    "    Work Style Preference: {row['Work_Style_Preference']}\n",
    "    Past Projects: {row['Past_Projects']}\n",
    "    Internships Completed: {row['Internships_Completed']}\n",
    "    Extracurriculars: {row['Extracurriculars']}\n",
    "    Career Outcome: {row['Career_Outcome']}\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = Document(\n",
    "        page_content=clean_text(content),\n",
    "        metadata={\n",
    "            \"source\": \"dataset.csv\",\n",
    "            \"type\": \"student_profile\",\n",
    "            \"student_id\": idx + 1,\n",
    "            \"degree_field\": row['Degree_Field'],\n",
    "            \"career_outcome\": row['Career_Outcome']\n",
    "        }\n",
    "    )\n",
    "    csv_documents.append(doc)\n",
    "\n",
    "print(f\"Created {len(csv_documents)} student profile documents\")\n",
    "\n",
    "# Create documents for theory content\n",
    "theory_documents = []\n",
    "\n",
    "# Holland Codes document\n",
    "holland_doc = Document(\n",
    "    page_content=holland_content,\n",
    "    metadata={\n",
    "        \"source\": \"holland-codes.txt\",\n",
    "        \"type\": \"career_theory\",\n",
    "        \"theory\": \"Holland RIASEC\"\n",
    "    }\n",
    ")\n",
    "theory_documents.append(holland_doc)\n",
    "\n",
    "# Lifespan theory document\n",
    "lifespan_doc = Document(\n",
    "    page_content=lifespan_content,\n",
    "    metadata={\n",
    "        \"source\": \"lifespan.txt\",\n",
    "        \"type\": \"career_theory\",\n",
    "        \"theory\": \"Super Life-Span Life-Space\"\n",
    "    }\n",
    ")\n",
    "theory_documents.append(lifespan_doc)\n",
    "\n",
    "print(f\"Created {len(theory_documents)} theory documents\")\n",
    "\n",
    "# Combine all documents\n",
    "all_documents = csv_documents + theory_documents\n",
    "print(f\"Total documents for RAG: {len(all_documents)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed659e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality analysis and validation\n",
    "print(\"\\n=== DATA QUALITY ANALYSIS ===\")\n",
    "\n",
    "# Analyze CSV data quality\n",
    "print(\"CSV Data Quality:\")\n",
    "print(f\"- Total records: {len(df_clean)}\")\n",
    "print(f\"- Complete records: {len(df_clean.dropna())}\")\n",
    "print(f\"- Records with missing values: {len(df_clean) - len(df_clean.dropna())}\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df_clean.duplicated().sum()\n",
    "print(f\"- Duplicate records: {duplicates}\")\n",
    "\n",
    "# Analyze degree field distribution\n",
    "print(f\"\\nDegree Field Distribution:\")\n",
    "degree_counts = df_clean['Degree_Field'].value_counts()\n",
    "print(degree_counts.head(10))\n",
    "\n",
    "# Analyze career outcomes\n",
    "print(f\"\\nCareer Outcome Distribution:\")\n",
    "career_counts = df_clean['Career_Outcome'].value_counts()\n",
    "print(career_counts.head(10))\n",
    "\n",
    "# Analyze text content quality\n",
    "print(f\"\\nText Content Quality:\")\n",
    "split_pattern = '\\n\\n'\n",
    "print(f\"- Holland Codes sections: {len(holland_content.split(split_pattern))}\")\n",
    "print(f\"- Lifespan Theory sections: {len(lifespan_content.split(split_pattern))}\")\n",
    "\n",
    "# Check document metadata\n",
    "print(f\"\\nDocument Metadata Summary:\")\n",
    "for i, doc in enumerate(all_documents[:3]):  # Show first 3 documents\n",
    "    print(f\"Document {i+1}: {doc.metadata}\")\n",
    "\n",
    "print(f\"\\n✅ Data cleaning and processing completed successfully!\")\n",
    "print(f\"📊 Ready for RAG model with {len(all_documents)} documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ab11d9",
   "metadata": {},
   "source": [
    "EMBEDDING AND VECTOR STORE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3f4fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/satyammishra/Desktop/RAG MODEL/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstall numpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchromadb\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "%pip install numpy  \n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Initialize the SentenceTransformer model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd49978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Embeddings:\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "        self.collection = None\n",
    "        self.index_name = \"rag_index\"\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07496aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
